{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from xbert.engine import Engine, weight_of_evidence, difference_of_log_probabilities, calculate_correlation\n",
    "from xbert import InputInstance, Config\n",
    "from xbert.visualization import visualize_relevances\n",
    "from xbert.occlusion.explainer import GradxInputExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "from segtok.tokenizer import web_tokenizer, space_tokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification  #, glue_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_DEVICE = 0 # or -1 if no GPU is available\n",
    "\n",
    "MODEL_NAME = \"roberta-large-mnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME).to(CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNLI_DATASET_PATH = \"../data/glue_data/MNLI/\"\n",
    "MNLI_IDX2LABEL = {0: 'contradiction', 1: 'neutral', 2: 'entailment'}\n",
    "MNLI_LABEL2IDX = {v: k for k, v in MNLI_IDX2LABEL.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rindex(alist, value):\n",
    "    return len(alist) - alist[-1::-1].index(value) - 1\n",
    "\n",
    "\n",
    "def byte_pair_offsets(input_ids, tokenizer):\n",
    "    def get_offsets(tokens, start_offset):\n",
    "        offsets = [start_offset]\n",
    "        for t_idx, token in enumerate(tokens, start_offset):\n",
    "            if not token.startswith(\" \"):\n",
    "                continue\n",
    "            offsets.append(t_idx)\n",
    "        offsets.append(start_offset + len(tokens))\n",
    "        return offsets\n",
    "        \n",
    "    tokens = [tokenizer.convert_tokens_to_string(t)\n",
    "              for t in tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=False)]\n",
    "    tokens = [token for token in tokens if token != \"<pad>\"]\n",
    "    tokens = tokens[1:-1]\n",
    "    \n",
    "    sent_1_end = tokens.index(\"</s>\")\n",
    "    sent_2_start = rindex(tokens, \"</s>\") + 1\n",
    "    \n",
    "    sent_1_offsets = get_offsets(tokens[:sent_1_end], start_offset=1)\n",
    "    sent_2_offsets = get_offsets(tokens[sent_2_start:], start_offset=sent_2_start+1)\n",
    "    \n",
    "    return sent_1_offsets, sent_2_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def read_mnli_dataset(path: str) -> List[Tuple[List[str], List[str], str]]:\n",
    "    dataset = []\n",
    "    with open(path) as fin:\n",
    "        fin.readline()\n",
    "        for index, line in enumerate(fin):\n",
    "            tokens = line.strip().split('\\t')\n",
    "            sent1, sent2, target = tokens[8], tokens[9], tokens[-1]\n",
    "            dataset.append((sent1, sent2, target))\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def dataset_to_input_instances(dataset: List[Tuple[List[str], List[str], str]]) -> List[InputInstance]:\n",
    "    input_instances = []\n",
    "    for idx, (sent1, sent2, _) in enumerate(dataset):\n",
    "        instance = InputInstance(id_=idx, sent1=web_tokenizer(sent1), sent2=web_tokenizer(sent2))\n",
    "        input_instances.append(instance)\n",
    "        \n",
    "    return input_instances\n",
    "\n",
    "\n",
    "def get_labels(dataset: List[Tuple[List[str], List[str], str]]) -> List[str]:\n",
    "    return [label for _, _, label in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tokens(values, pad_idx, eos_idx=None, left_pad=False, move_eos_to_beginning=False):\n",
    "    \"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"\n",
    "    size = max(v.size(0) for v in values)\n",
    "    res = values[0].new(len(values), size).fill_(pad_idx)\n",
    "    \n",
    "    def copy_tensor(src, dst):\n",
    "        assert dst.numel() == src.numel()\n",
    "        if move_eos_to_beginning:\n",
    "            assert src[-1] == eos_idx\n",
    "            dst[0] = eos_idx\n",
    "            dst[1:] = src[:-1]\n",
    "        else:\n",
    "            dst.copy_(src)\n",
    "\n",
    "    for i, v in enumerate(values):\n",
    "        copy_tensor(v, res[i][size - len(v):] if left_pad else res[i][:len(v)])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_instance(input_instance):\n",
    "    return tokenizer.encode(text=\" \".join(input_instance.sent1.tokens),\n",
    "                            text_pair=\" \".join(input_instance.sent2.tokens),\n",
    "                            add_special_tokens=True,\n",
    "                            return_tensors=\"pt\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def predict(input_instance, model, tokenizer, cuda_device):\n",
    "#    input_ids = tokenizer.encode(text=input_instance.sent1.tokens,\n",
    "#                                 text_pair=input_instance.sent2.tokens,\n",
    "#                                 add_special_tokens=True,\n",
    "#                                 return_tensors=\"pt\").to(cuda_device)\n",
    "#    \n",
    "#    logits = model(input_ids)[0]\n",
    "#    return F.softmax(logits, dim=-1)\n",
    "\n",
    "def predict(input_instances, model, tokenizer, cuda_device):\n",
    "    if isinstance(input_instances, InputInstance):\n",
    "        input_instances = [input_instances]\n",
    "    \n",
    "    input_ids = [encode_instance(instance) for instance in input_instances]\n",
    "    attention_mask = [torch.ones_like(t) for t in input_ids]\n",
    "    \n",
    "    input_ids = collate_tokens(input_ids, pad_idx=1).to(cuda_device)\n",
    "    attention_mask = collate_tokens(attention_mask, pad_idx=0).to(cuda_device)\n",
    "    \n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "    return F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_mnli_dataset(os.path.join(MNLI_DATASET_PATH, \"dev_matched.tsv\"))\n",
    "input_instances = dataset_to_input_instances(dataset)\n",
    "labels = get_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 10/98 [00:18<02:43,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Accuracy:  0.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "ncorrect, nsamples = 0, 0\n",
    "for i in tqdm(range(0, len(input_instances[:1000]), batch_size), total=len(input_instances) // batch_size):\n",
    "    batch_instances = input_instances[i: i + batch_size]\n",
    "    with torch.no_grad():\n",
    "        probs = predict(batch_instances, model, tokenizer, CUDA_DEVICE)\n",
    "        #print(probs)\n",
    "        predictions = probs.argmax(dim=-1).cpu().numpy().tolist()\n",
    "        #print(predictions)\n",
    "        for batch_idx, instance in enumerate(batch_instances):\n",
    "            # the instance id is also the position in the list of labels\n",
    "            idx = instance.id\n",
    "            true_label = labels[idx]\n",
    "            pred_label = MNLI_IDX2LABEL[predictions[batch_idx]]\n",
    "            ncorrect += int(true_label == pred_label)\n",
    "            nsamples += 1\n",
    "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(batch_instances):\n",
    "    true_label_indices = []\n",
    "    probabilities = []\n",
    "    with torch.no_grad():\n",
    "        probs = predict(batch_instances, model, tokenizer, CUDA_DEVICE).cpu().numpy().tolist()\n",
    "        for batch_idx, instance in enumerate(batch_instances):\n",
    "            # the instance id is also the position in the list of labels\n",
    "            idx = instance.id\n",
    "            true_label_idx = MNLI_LABEL2IDX[labels[idx]]\n",
    "            true_label_indices.append(true_label_idx)\n",
    "            probabilities.append(probs[batch_idx][true_label_idx])\n",
    "    \n",
    "    return probabilities\n",
    "    \n",
    "    \n",
    "def batcher_gradient(batch_instances):\n",
    "    input_ids = collate_tokens(\n",
    "        [encode_instance(instance) for instance in batch_instances], pad_idx=1\n",
    "    ).to(CUDA_DEVICE)\n",
    "\n",
    "    device = input_ids.device\n",
    "    input_shape = input_ids.size()\n",
    "\n",
    "    token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "    inp = model.roberta.embeddings(input_ids=input_ids).detach()\n",
    "    \n",
    "    explainer = GradxInputExplainer(model, output_getter=lambda x: x[0])\n",
    "    inp.requires_grad = True\n",
    "    expl = explainer.explain(inp)\n",
    "    \n",
    "    input_ids_np = input_ids.cpu().numpy()\n",
    "    expl_np = expl.cpu().numpy()\n",
    "\n",
    "    relevances = []\n",
    "    for b_idx in range(input_ids_np.shape[0]):\n",
    "        sent1_offsets, sent2_offsets = byte_pair_offsets(input_ids_np[b_idx].tolist(), tokenizer)\n",
    "        \n",
    "        relevance_dict = defaultdict(float)\n",
    "        for offsets, sent_id in zip([sent1_offsets, sent2_offsets], [\"sent1\", \"sent2\"]):\n",
    "            for token_idx, (token_start, token_end) in enumerate(zip(offsets, offsets[1:])):\n",
    "                relevance = expl_np[b_idx][token_start: token_end].sum()\n",
    "                relevance_dict[(sent_id, token_idx)] = relevance\n",
    "        relevances.append(relevance_dict)\n",
    "\n",
    "    return relevances\n",
    "    \n",
    "\n",
    "config_unk = Config.from_dict({\n",
    "    \"strategy\": \"unk_replacement\",\n",
    "    \"batch_size\": 128,\n",
    "    \"unk_token\": \"<unk>\"\n",
    "})\n",
    "\n",
    "config_gradient = Config.from_dict({\n",
    "    \"strategy\": \"gradient\",\n",
    "    \"batch_size\": 128\n",
    "})\n",
    "\n",
    "config_resample = Config.from_dict({\n",
    "    \"strategy\": \"bert_lm_sampling\",\n",
    "    \"cuda_device\": 0,\n",
    "    \"bert_model\": \"bert-base-uncased\",\n",
    "    \"batch_size\": 256,\n",
    "    \"n_samples\": 100,\n",
    "    \"verbose\": False\n",
    "})\n",
    "\n",
    "unknown_engine = Engine(config_unk, batcher)\n",
    "resample_engine = Engine(config_resample, batcher)\n",
    "gradient_engine = Engine(config_gradient, batcher_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_idx = 0\n",
    "n = 2\n",
    "\n",
    "unk_candidate_instances, unk_candidate_results = unknown_engine.run(input_instances[instance_idx: instance_idx+n])\n",
    "res_candidate_instances, res_candidate_results = resample_engine.run(input_instances[instance_idx: instance_idx+n])\n",
    "grad_candidate_instances, grad_candidate_results = gradient_engine.run(input_instances[instance_idx: instance_idx+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_relevances = unknown_engine.relevances(unk_candidate_instances, unk_candidate_results)\n",
    "res_relevances = resample_engine.relevances(res_candidate_instances, res_candidate_results)\n",
    "grad_relevances = gradient_engine.relevances(grad_candidate_instances, grad_candidate_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true = labels[instance_idx: instance_idx+n]\n",
    "labels_pred = [MNLI_IDX2LABEL[predict(instance, model, tokenizer, CUDA_DEVICE)[0].argmax().item()] for instance in input_instances[instance_idx: instance_idx+n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">neutral &#10004; 0.40</span>:   <span style=\"color:black; background-color:rgba(0, 0, 255, 0.00458536259094665);\">The</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.12300527034801817);\">new</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.00803440997435855);\">rights</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.017127447243248106);\">are</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.020022024893279006);\">nice</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.007979954446767355);\">enough</span></font></br><font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">neutral &#10004; 0.40</span>:   <span style=\"color:white; background-color:rgba(255, 0, 0, 1.0);\">Everyone</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.020157571804348393);\">really</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.2787409763639334);\">likes</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.029219237954082152);\">the</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.016578156704067353);\">newest</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0746378115507277);\">benefits</span></font></br><font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">contradiction &#10004; 1.00</span>:   <span style=\"color:black; background-color:rgba(255, 0, 0, 6.478583393151179e-05);\">This</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 7.529487243100125e-05);\">site</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 3.3437849771102855e-05);\">includes</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 2.7466805169120202e-06);\">a</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 7.123456210165305e-05);\">list</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 4.657414789546469e-06);\">of</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 4.340949425641389e-05);\">all</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 9.243177043869146e-05);\">award</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 4.00657092793036e-05);\">winners</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 2.686970070892194e-05);\">and</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.09576444927284207);\">a</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.19892194741962896);\">searchable</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.00012097336363616855);\">database</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.00019764157632562581);\">of</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0006127485970554599);\">Government</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.014317669298864105);\">Executive</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 7.839981562403223e-05);\">articles</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 3.0332906578071877e-05);\">.</span></font></br><font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">contradiction &#10004; 1.00</span>:   <span style=\"color:black; background-color:rgba(0, 0, 255, 2.3346784393752174e-05);\">The</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0003098972148428997);\">Government</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.00038752079466867416);\">Executive</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0006372895903696085);\">articles</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.00011882378757945479);\">housed</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.00011488289814214624);\">on</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.00010317965072226024);\">the</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 7.422008440264438e-05);\">website</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 1.3016877232322184e-05);\">are</span> <span style=\"color:white; background-color:rgba(255, 0, 0, 1.0);\">not</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0006029560839082083);\">able</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 2.6392017140763327e-05);\">to</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0003135992624961289);\">be</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.00021274831916864192);\">searched</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 1.3852823476599754e-05);\">.</span></font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(visualize_relevances(input_instances[instance_idx: instance_idx+n], unk_relevances, labels_true, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">neutral &#10004; 0.31</span>:   <span style=\"color:black; background-color:rgba(255, 0, 0, 0.03078855911065214);\">The</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.3946441368090235);\">new</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.13879366045007016);\">rights</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.021349437189510614);\">are</span> <span style=\"color:white; background-color:rgba(255, 0, 0, 1.0);\">nice</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.03871130321317243);\">enough</span></font></br><font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">neutral &#10004; 0.31</span>:   <span style=\"color:black; background-color:rgba(255, 0, 0, 0.5446082131817856);\">Everyone</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.028940813583755436);\">really</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.1573696933050874);\">likes</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0005583938639653688);\">the</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.021153654082103842);\">newest</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.05549149498978667);\">benefits</span></font></br><font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">contradiction &#10004; 0.85</span>:   <span style=\"color:black; background-color:rgba(255, 0, 0, 2.6393389996489834e-05);\">This</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 3.595886922280246e-05);\">site</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 6.515117505117e-06);\">includes</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 4.551311064701636e-07);\">a</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 7.421165541109226e-06);\">list</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0);\">of</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 1.1285565768895866e-05);\">all</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 4.4136479519358346e-06);\">award</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 2.105683730129267e-06);\">winners</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0);\">and</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 2.268631904322025e-07);\">a</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.13961664050461298);\">searchable</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 6.751884011279121e-05);\">database</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 2.8916277240818007e-06);\">of</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.2843767190478161);\">Government</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.1684929968476009);\">Executive</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0005482419405720382);\">articles</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0);\">.</span></font></br><font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">contradiction &#10004; 0.85</span>:   <span style=\"color:black; background-color:rgba(255, 0, 0, 0.08247972161037437);\">The</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.11033649718194999);\">Government</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0518801131754646);\">Executive</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0196266137864192);\">articles</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.00014010241851030898);\">housed</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 3.7787119640038092e-06);\">on</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 3.2835461768537484e-06);\">the</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.18254975232510767);\">website</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 9.911744095719997e-06);\">are</span> <span style=\"color:white; background-color:rgba(255, 0, 0, 1.0);\">not</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.04781141526988113);\">able</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0);\">to</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0);\">be</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.058182648501693614);\">searched</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 1.917450495032854e-07);\">.</span></font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(visualize_relevances(input_instances[instance_idx: instance_idx+n], res_relevances, labels_true, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">neutral &#10004; 0.02</span>:   <span style=\"color:black; background-color:rgba(255, 0, 0, 0.10920654982328415);\">The</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.06476353853940964);\">new</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.16908234357833862);\">rights</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.029866091907024384);\">are</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.10879836976528168);\">nice</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.04174195975065231);\">enough</span></font></br><font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">neutral &#10004; 0.02</span>:   <span style=\"color:white; background-color:rgba(255, 0, 0, 1.0);\">Everyone</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.07928569614887238);\">really</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.07259906828403473);\">likes</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.1932590901851654);\">the</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.4096910059452057);\">newest</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.1747337281703949);\">benefits</span></font></br><font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">contradiction &#10004; 0.00</span>:   <span style=\"color:black; background-color:rgba(255, 0, 0, 0.09514252096414566);\">This</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.20972111821174622);\">site</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.06985262781381607);\">includes</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.01897842064499855);\">a</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0160245168954134);\">list</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.015129808336496353);\">of</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.019502822309732437);\">all</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.13489869236946106);\">award</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.1558552086353302);\">winners</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.021704336628317833);\">and</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.00995921716094017);\">a</span> <span style=\"color:white; background-color:rgba(255, 0, 0, 1.0);\">searchable</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.027289874851703644);\">database</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.08432959765195847);\">of</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.018106713891029358);\">Government</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.08860708028078079);\">Executive</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.1771222949028015);\">articles</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.05428994446992874);\">.</span></font></br><font size=\"5\"><span style=\"color:black; background-color:rgba(255, 255, 0, 0.6);\">contradiction &#10004; 0.00</span>:   <span style=\"color:black; background-color:rgba(255, 0, 0, 0.04991421848535538);\">The</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.08422532677650452);\">Government</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.0949520692229271);\">Executive</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.21368323266506195);\">articles</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.07648342847824097);\">housed</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.025166628882288933);\">on</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.041721198707818985);\">the</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.013507087714970112);\">website</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.05240628123283386);\">are</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.2472897171974182);\">not</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.2980249226093292);\">able</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.20562323927879333);\">to</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.03734452649950981);\">be</span> <span style=\"color:black; background-color:rgba(255, 0, 0, 0.008090953342616558);\">searched</span> <span style=\"color:black; background-color:rgba(0, 0, 255, 0.14571699500083923);\">.</span></font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(visualize_relevances(input_instances[instance_idx: instance_idx+n], grad_relevances, labels_true, labels_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6490628562805311\n",
      "0.366195462088272\n",
      "0.0921416301964605\n"
     ]
    }
   ],
   "source": [
    "print(calculate_correlation(unk_relevances, res_relevances))\n",
    "print(calculate_correlation(unk_relevances, grad_relevances))\n",
    "print(calculate_correlation(res_relevances, grad_relevances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
